Metadata-Version: 2.2
Name: grpoptim
Version: 0.1.0
Summary: Group Relative Policy Optimization for Efficient RL Training
Home-page: https://github.com/subaashnair/grpoptim
Author: Your Name
Author-email: your.email@example.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: POSIX :: Linux
Classifier: Operating System :: MacOS :: MacOS X
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: numpy>=1.23.0
Provides-Extra: test
Requires-Dist: pytest>=7.0; extra == "test"
Requires-Dist: torch>=1.10; extra == "test"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# **GRPOptim** üöÄ  
**Group Relative Policy Optimization for Efficient Reinforcement Learning**  

[![PyPI Version](https://img.shields.io/pypi/v/GRPOptim.svg)](https://pypi.org/project/GRPOptim/)  
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)  
[![Python Versions](https://img.shields.io/pypi/pyversions/GRPOptim.svg)](https://pypi.org/project/GRPOptim/)  
[![Downloads](https://static.pepy.tech/badge/GRPOptim/month)](https://pepy.tech/project/GRPOptim)  
[![Build Status](https://github.com/subaashnair/GRPOptim/actions/workflows/tests.yml/badge.svg)](https://github.com/subaashnair/GRPOptim/actions)  

---

### **What is GRPOptim?**  
**GRPOptim** is a lightweight Python library for training reinforcement learning (RL) agents using **Group Relative Policy Optimization (GRPO)**, a critic-free algorithm that reduces training costs by leveraging group-based advantage estimation and KL regularization. Ideal for tasks like language model alignment, robotics, and game AI.  

---

### **Key Features**  
- üö´ **No Critic Model**: Trains policies directly with 50% fewer parameters.  
- üìä **Group-Based Advantage**: Stabilizes training using reward standardization.  
- üîß **PyTorch/TensorFlow Compatible**: Easy integration with your ML pipeline.  
- üêç **Pythonic API**: Minimal boilerplate for RL practitioners.  

---

### **Installation**  
```bash  
pip install GRPOptim  
```  

---

### **Quick Example**  
```python  
import GRPOptim as gro  

# Initialize GRPO trainer  
trainer = gro.GRPO(  
    policy_model=your_policy_network,  
    reference_model=your_reference_network,  
    epsilon=0.2,  
    beta=0.1  
)  

# Train with group sampling  
for epoch in range(epochs):  
    group = trainer.sample_group(inputs)  
    rewards = compute_rewards(group)  
    loss = trainer.step(group, rewards)  
```  

---

### **Why GRPOptim?**  
- üèéÔ∏è **Faster Training**: Skip the critic network and train policies directly.  
- üìâ **Stable Convergence**: Group normalization and clipping prevent reward collapse.  
- üß© **Drop-In Replacement**: Compatible with RL workflows (PPO, A2C, etc.).  

---

### **License**  
MIT License. See [LICENSE](LICENSE).  

---

### **Name Justification**  
- **GRPO**: Direct reference to the algorithm.  
- **Optim**: Short for "optimization," emphasizing efficiency.  
- **Unique & Memorable**: Easy to search for on PyPI/GitHub.  

---
